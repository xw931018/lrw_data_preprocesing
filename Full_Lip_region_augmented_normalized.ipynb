{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PATH='$PATH:/home/monga/.local/lib/python3.8/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import util\n",
    "from psfdataset import PSFDataset, transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import util_lip_data as util_lip\n",
    "import custom_transforms as ctransform\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 THOUGHT\n",
      "2 INVOLVED\n",
      "3 DIFFERENT\n",
      "4 MAYBE\n",
      "5 REPORT\n",
      "6 VICTIMS\n",
      "7 GETTING\n",
      "8 UNION\n",
      "9 MEDIA\n",
      "10 VIOLENCE\n",
      "-------------------------------\n",
      "dict_keys(['THOUGHT', 'INVOLVED', 'DIFFERENT', 'MAYBE', 'REPORT', 'VICTIMS', 'GETTING', 'UNION', 'MEDIA', 'VIOLENCE'])\n",
      "dict_keys(['val', 'test', 'train'])\n",
      "1000\n",
      "(29, 20, 2)\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "script to compute the number\n",
    "of jsons created for each\n",
    "word. The output is a .csv\n",
    "file where for each word the\n",
    "number of jsons corresponding\n",
    "to train, val, and test subsets\n",
    "is mentioned\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_rect_and_landmarks(rect, landmarks):\n",
    "    # converts and returns the face rectangle and landmarks\n",
    "    # in formats appropriate for the display function\n",
    "\n",
    "    x = rect[\"left\"]\n",
    "    y = rect[\"top\"]\n",
    "    w = rect[\"width\"]\n",
    "    h = rect[\"height\"]\n",
    "\n",
    "    if landmarks is not None:\n",
    "        temp_agg = list()\n",
    "        for i in range(len(landmarks)):\n",
    "            temp = list()\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"x\"])\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"y\"])\n",
    "            temp_agg.append(temp)\n",
    "        return (x, y, w, h), np.asarray(temp_agg)\n",
    "    else:\n",
    "        return (x, y, w, h), np.empty((0, 0))\n",
    "\n",
    "\n",
    "def choose_the_largest_face(faces_list):\n",
    "    if len(faces_list) == 1: \n",
    "        return faces_list[0]\n",
    "    \n",
    "    area_max = 0\n",
    "    area_max_id = 0\n",
    "    for i,face in enumerate(faces_list):\n",
    "        (face_rect,landmarks) = face\n",
    "        area = face_rect[2] * face_rect[3] # area = width * height\n",
    "        if area > area_max:\n",
    "            area_max = area\n",
    "            area_max_id = i\n",
    "    return faces_list[area_max_id]\n",
    "\n",
    "\n",
    "def load_one_json_file(filename, isDebug=False):\n",
    "    # load the metadata and facial landmarks\n",
    "\n",
    "    face_rect_list = []\n",
    "    landmarks_list = []\n",
    "    with open(filename) as f:\n",
    "        video_data_dict = json.load(f)\n",
    "        # extract duration\n",
    "        if video_data_dict[\"metaData\"] is not None:\n",
    "            duration = float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", video_data_dict[\"metaData\"][\"Duration\"])[0])\n",
    "            if isDebug:\n",
    "                print(\"duration metadata: %.3f\" % duration)\n",
    "\n",
    "        # extract frame information aggregated for all frames\n",
    "        agg_frame_data = video_data_dict[\"aggFrameInfo\"]  # list of frame-wise visual data\n",
    "\n",
    "        for frame_data in agg_frame_data:\n",
    "            n_faces = frame_data[\"numFaces\"]\n",
    "            if isDebug:\n",
    "                print(\"frame index: %d number of faces: %d\" % (frame_data[\"frameIndex\"], n_faces))\n",
    "            \n",
    "            if frame_data[\"facialAttributes\"] is not None:# if so, the n_faces should > 0 \n",
    "                faces_list = []\n",
    "                for attr in frame_data[\"facialAttributes\"]:\n",
    "                    face_idx = attr[\"faceIndex\"]\n",
    "                    face_rect, landmarks = get_rect_and_landmarks(attr[\"faceRectangle\"],\n",
    "                                                                  attr[\"faceLandmarks\"])\n",
    "                    faces_list.append((face_rect, landmarks))\n",
    "\n",
    "                face_rect_chosen, landmarks_chosen = choose_the_largest_face(faces_list)    \n",
    "                face_rect_list.append(face_rect_chosen)\n",
    "                landmarks_list.append(landmarks_chosen)\n",
    "    \n",
    "    face_rect_array = np.array(face_rect_list)\n",
    "    landmarks_array = np.array(landmarks_list)\n",
    "    return face_rect_array, landmarks_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json/lipread_mp4\"\n",
    "# or i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json_defects_not_one_face/lipread_mp4\"\n",
    "selected_n_classes = 10 # the max is 500\n",
    "\n",
    "cnt = 0\n",
    "data = dict()\n",
    "\n",
    "for word in os.listdir(i_data):\n",
    "    if not word.startswith('.'):\n",
    "        cnt += 1\n",
    "        if cnt > selected_n_classes:\n",
    "            break\n",
    "        print(cnt,word)\n",
    "        splits = dict() # 'train' 'val' and 'test' sets\n",
    "        # print(\"analysing data for the word: '%s'\" % word)\n",
    "        p = os.path.join(i_data, word)\n",
    "        \n",
    "        for sub_dir in os.listdir(p):\n",
    "            if not sub_dir.startswith('.'):\n",
    "                # print(sub_dir)\n",
    "                p_sub = os.path.join(p, sub_dir)\n",
    "                for _, _, files in os.walk(p_sub):\n",
    "                    samples_list = []\n",
    "                    for filename in files:\n",
    "                        if filename.endswith('.json'):\n",
    "                            face_rect_array, landmarks_array = load_one_json_file(os.path.join(p_sub, filename))\n",
    "                            lip_region = []\n",
    "                            for j in range(len(landmarks_array)):\n",
    "                                lip_region.append(landmarks_array[j][48:68])\n",
    "                            samples_list.append(np.array(lip_region))\n",
    "                    splits[sub_dir] = samples_list\n",
    "        data[word] = splits\n",
    "\n",
    "print('-------------------------------')\n",
    "print(data.keys()) # names of all the 'selected_n_classes' classes  \n",
    "print(data['THOUGHT'].keys()) # print the names of the 3 splits for the first class 'THOUGHT'\n",
    "print(len(data['THOUGHT']['train'])) # print the number of train samples of the first class\n",
    "print(data['THOUGHT']['train'][0].shape) # print the shape (29 frames, 68 landmarks, 2 coordinates) of the first training sample of the first class    \n",
    "print('-------------------------------')                     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Defining a LabelEncoder to transform text class to numberic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder= LabelEncoder()\n",
    "categories = list(data.keys())\n",
    "encoder.fit(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining my custom Data loader from above 'data'. We only need to properly define iterators for train/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iterator(data, key = 'train', refLength = 29):\n",
    "    iter_list = []\n",
    "    x = 0\n",
    "    for word, keypointsOneWord in data.items():\n",
    "        \n",
    "\n",
    "        keypointsList = keypointsOneWord[key]\n",
    "        num_of_samples = len(keypointsList)\n",
    "        \n",
    "        ## There are some sample whose length is smaller than 29. We need to either delete it, \n",
    "        ## or extend it to 29 length for now. But this can be resolved if signature transform is introduced\n",
    "        for i in range(len(keypointsList)):\n",
    "             singleSample = keypointsList[i]\n",
    "             if len(singleSample) < refLength:\n",
    "                 singleSample = np.array(list(singleSample) + [singleSample[-1]] * (refLength - len(singleSample)))\n",
    "                 keypointsList[i] = singleSample\n",
    "        iter_list = iter_list + list(zip(keypointsList, np.array(list(encoder.transform([word])) * num_of_samples)))\n",
    "    return iter(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = generate_iterator(data, key = 'train')\n",
    "iter_test = generate_iterator(data, key = 'test')\n",
    "iter_val = generate_iterator(data, key = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiter_test = [x for x in iter_test]\n",
    "iiter_train = [x for x in iter_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Let's adapt the PSFDataset from human body movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([\n",
    "    #transforms.spatial.Crop(),\n",
    "#     transforms.spatial.Normalize(),\n",
    "#     transforms.SpatioTemporalPath(),\n",
    "#     transforms.temporal.MultiDelayedTransformation(2),\n",
    "    transforms.temporal.DyadicPathSignatures(dyadic_levels=2,\n",
    "                                             signature_level=4)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first steps, no transforms are introduced yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new dictionaries identical to 'data' that contain the normalized landmarks without aumenting \n",
    "#the training set\n",
    "\n",
    "    \n",
    "d_normalize = {} # normalized data set\n",
    "for i in data: \n",
    "    d_normalize[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in data[i].items())\n",
    "    \n",
    "d_rotate = {}    # rotated data set \n",
    "for i in data: \n",
    "    d_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate= {} # normalized and rotated data set\n",
    "for i in d_normalize:\n",
    "    d_normalize_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize[i].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new dictionary identical to 'data' that contains the normalized landmarks augmenting \n",
    "#the training set\n",
    "\n",
    "aug_data = ctransform.flip_train_augmentation(data)\n",
    "\n",
    "d_normalize_a = {} #normalized data set\n",
    "for i in data: \n",
    "    d_normalize_a[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in aug_data[i].items())\n",
    "    \n",
    "d_rotate_a = {}    #rotated data set \n",
    "for i in data: \n",
    "    d_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in aug_data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate_a= {} ## normalized and rotated data set\n",
    "for i in d_normalize_a:\n",
    "    d_normalize_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize_a[i].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [02:14, 149.03it/s]\n",
      "500it [00:03, 149.19it/s]\n",
      "500it [00:03, 148.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainingset elements: 20000\n",
      "Number of testset elements 500\n",
      "Dimension of feature vector: 6293\n",
      "Initial accuracy: 0.102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b871328e7e5849d49cf6569e3aaf4dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=20.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541bda700fa746f895687d272f7c70b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 0', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 2.1637766412943784 \ttest_loss: 1.7139191680466157 \tAccuracy: 0.428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03b13e38b5b408698def0cbfb714e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 1', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.9404372387356617 \ttest_loss: 1.5144053529307582 \tAccuracy: 0.418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9553c5ad7ee74c1998909753b3fc2c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 2', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.8441097809181268 \ttest_loss: 1.3383336859928423 \tAccuracy: 0.524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f19f487431243f08f3c22a24c8faba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 3', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.787271584258281 \ttest_loss: 1.3505610709775158 \tAccuracy: 0.514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914bf8e62c1a4641acb6ea022b6cfeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 4', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.7462355679933095 \ttest_loss: 1.300288894917711 \tAccuracy: 0.558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b20ddeea7a6451fbfca802475bee0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 5', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.7150032283076466 \ttest_loss: 1.238021396428073 \tAccuracy: 0.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01fa9f5518a43a1a9b4e9c909c74cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 6', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.6894202260746882 \ttest_loss: 1.2691704201729463 \tAccuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ff476dcbb494292fcd6ecdb8c8712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 7', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.6676333988130159 \ttest_loss: 1.269545850241953 \tAccuracy: 0.572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f98cefcbe9d49cc8b92d43098d0ac93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 8', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.64878609614302 \ttest_loss: 1.2580395196006549 \tAccuracy: 0.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a68a29ea2b49af85d155d94192ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 9', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.6299883146408538 \ttest_loss: 1.1562372115835449 \tAccuracy: 0.618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530b1ae2d39f493fba6d7ccdb5cd6b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 10', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.6134503598358896 \ttest_loss: 1.201282261000923 \tAccuracy: 0.598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37178d96ba214762bdd309f7f233a9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 11', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.591812501442287 \ttest_loss: 1.1829582679948951 \tAccuracy: 0.602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c09e8068f8d425d98971871e13b40eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 12', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.5795645371858333 \ttest_loss: 1.1804752549413617 \tAccuracy: 0.612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080c3740b1d5432f9951e7a7bfee1b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 13', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.5635181482617373 \ttest_loss: 1.1912194467780106 \tAccuracy: 0.614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c69487a35141198c9bbc3e457d7fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 14', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.5523862303794251 \ttest_loss: 1.1110912067771157 \tAccuracy: 0.632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532e7ccd424c4406be365647e5a5ce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 15', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.538155022181702 \ttest_loss: 1.1413518639474225 \tAccuracy: 0.632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40bccbf0d5c4e17af4acc03b07a2093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 16', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.5236776708015298 \ttest_loss: 1.084215129808067 \tAccuracy: 0.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5cc05c8faf4e56bc0d6379fd6c4de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 17', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, outputs = ctransform.trainModelWithSpecificDataDict(dataDict = d_normalize_a, transforms = tr, EPOCHS = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
