{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PATH='$PATH:/home/monga/.local/lib/python3.8/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import util\n",
    "from psfdataset import PSFDataset, transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import util_lip_data as util_lip\n",
    "import custom_transforms as ctransform\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 THOUGHT\n",
      "2 INVOLVED\n",
      "3 DIFFERENT\n",
      "4 MAYBE\n",
      "5 REPORT\n",
      "6 VICTIMS\n",
      "7 GETTING\n",
      "8 UNION\n",
      "9 MEDIA\n",
      "10 VIOLENCE\n",
      "-------------------------------\n",
      "dict_keys(['THOUGHT', 'INVOLVED', 'DIFFERENT', 'MAYBE', 'REPORT', 'VICTIMS', 'GETTING', 'UNION', 'MEDIA', 'VIOLENCE'])\n",
      "dict_keys(['val', 'test', 'train'])\n",
      "1000\n",
      "(29, 20, 2)\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "script to compute the number\n",
    "of jsons created for each\n",
    "word. The output is a .csv\n",
    "file where for each word the\n",
    "number of jsons corresponding\n",
    "to train, val, and test subsets\n",
    "is mentioned\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_rect_and_landmarks(rect, landmarks):\n",
    "    # converts and returns the face rectangle and landmarks\n",
    "    # in formats appropriate for the display function\n",
    "\n",
    "    x = rect[\"left\"]\n",
    "    y = rect[\"top\"]\n",
    "    w = rect[\"width\"]\n",
    "    h = rect[\"height\"]\n",
    "\n",
    "    if landmarks is not None:\n",
    "        temp_agg = list()\n",
    "        for i in range(len(landmarks)):\n",
    "            temp = list()\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"x\"])\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"y\"])\n",
    "            temp_agg.append(temp)\n",
    "        return (x, y, w, h), np.asarray(temp_agg)\n",
    "    else:\n",
    "        return (x, y, w, h), np.empty((0, 0))\n",
    "\n",
    "\n",
    "def choose_the_largest_face(faces_list):\n",
    "    if len(faces_list) == 1: \n",
    "        return faces_list[0]\n",
    "    \n",
    "    area_max = 0\n",
    "    area_max_id = 0\n",
    "    for i,face in enumerate(faces_list):\n",
    "        (face_rect,landmarks) = face\n",
    "        area = face_rect[2] * face_rect[3] # area = width * height\n",
    "        if area > area_max:\n",
    "            area_max = area\n",
    "            area_max_id = i\n",
    "    return faces_list[area_max_id]\n",
    "\n",
    "\n",
    "def load_one_json_file(filename, isDebug=False):\n",
    "    # load the metadata and facial landmarks\n",
    "\n",
    "    face_rect_list = []\n",
    "    landmarks_list = []\n",
    "    with open(filename) as f:\n",
    "        video_data_dict = json.load(f)\n",
    "        # extract duration\n",
    "        if video_data_dict[\"metaData\"] is not None:\n",
    "            duration = float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", video_data_dict[\"metaData\"][\"Duration\"])[0])\n",
    "            if isDebug:\n",
    "                print(\"duration metadata: %.3f\" % duration)\n",
    "\n",
    "        # extract frame information aggregated for all frames\n",
    "        agg_frame_data = video_data_dict[\"aggFrameInfo\"]  # list of frame-wise visual data\n",
    "\n",
    "        for frame_data in agg_frame_data:\n",
    "            n_faces = frame_data[\"numFaces\"]\n",
    "            if isDebug:\n",
    "                print(\"frame index: %d number of faces: %d\" % (frame_data[\"frameIndex\"], n_faces))\n",
    "            \n",
    "            if frame_data[\"facialAttributes\"] is not None:# if so, the n_faces should > 0 \n",
    "                faces_list = []\n",
    "                for attr in frame_data[\"facialAttributes\"]:\n",
    "                    face_idx = attr[\"faceIndex\"]\n",
    "                    face_rect, landmarks = get_rect_and_landmarks(attr[\"faceRectangle\"],\n",
    "                                                                  attr[\"faceLandmarks\"])\n",
    "                    faces_list.append((face_rect, landmarks))\n",
    "\n",
    "                face_rect_chosen, landmarks_chosen = choose_the_largest_face(faces_list)    \n",
    "                face_rect_list.append(face_rect_chosen)\n",
    "                landmarks_list.append(landmarks_chosen)\n",
    "    \n",
    "    face_rect_array = np.array(face_rect_list)\n",
    "    landmarks_array = np.array(landmarks_list)\n",
    "    return face_rect_array, landmarks_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json/lipread_mp4\"\n",
    "# or i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json_defects_not_one_face/lipread_mp4\"\n",
    "selected_n_classes = 10 # the max is 500\n",
    "\n",
    "cnt = 0\n",
    "data = dict()\n",
    "\n",
    "for word in os.listdir(i_data):\n",
    "    if not word.startswith('.'):\n",
    "        cnt += 1\n",
    "        if cnt > selected_n_classes:\n",
    "            break\n",
    "        print(cnt,word)\n",
    "        splits = dict() # 'train' 'val' and 'test' sets\n",
    "        # print(\"analysing data for the word: '%s'\" % word)\n",
    "        p = os.path.join(i_data, word)\n",
    "        \n",
    "        for sub_dir in os.listdir(p):\n",
    "            if not sub_dir.startswith('.'):\n",
    "                # print(sub_dir)\n",
    "                p_sub = os.path.join(p, sub_dir)\n",
    "                for _, _, files in os.walk(p_sub):\n",
    "                    samples_list = []\n",
    "                    for filename in files:\n",
    "                        if filename.endswith('.json'):\n",
    "                            face_rect_array, landmarks_array = load_one_json_file(os.path.join(p_sub, filename))\n",
    "                            lip_region = []\n",
    "                            for j in range(len(landmarks_array)):\n",
    "                                lip_region.append(landmarks_array[j][48:68])\n",
    "                            samples_list.append(np.array(lip_region))\n",
    "                    splits[sub_dir] = samples_list\n",
    "        data[word] = splits\n",
    "\n",
    "print('-------------------------------')\n",
    "print(data.keys()) # names of all the 'selected_n_classes' classes  \n",
    "print(data['THOUGHT'].keys()) # print the names of the 3 splits for the first class 'THOUGHT'\n",
    "print(len(data['THOUGHT']['train'])) # print the number of train samples of the first class\n",
    "print(data['THOUGHT']['train'][0].shape) # print the shape (29 frames, 68 landmarks, 2 coordinates) of the first training sample of the first class    \n",
    "print('-------------------------------')                     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Defining a LabelEncoder to transform text class to numberic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder= LabelEncoder()\n",
    "categories = list(data.keys())\n",
    "encoder.fit(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining my custom Data loader from above 'data'. We only need to properly define iterators for train/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iterator(data, key = 'train', refLength = 29):\n",
    "    iter_list = []\n",
    "    x = 0\n",
    "    for word, keypointsOneWord in data.items():\n",
    "        \n",
    "\n",
    "        keypointsList = keypointsOneWord[key]\n",
    "        num_of_samples = len(keypointsList)\n",
    "        \n",
    "        ## There are some sample whose length is smaller than 29. We need to either delete it, \n",
    "        ## or extend it to 29 length for now. But this can be resolved if signature transform is introduced\n",
    "        for i in range(len(keypointsList)):\n",
    "             singleSample = keypointsList[i]\n",
    "             if len(singleSample) < refLength:\n",
    "                 singleSample = np.array(list(singleSample) + [singleSample[-1]] * (refLength - len(singleSample)))\n",
    "                 keypointsList[i] = singleSample\n",
    "        iter_list = iter_list + list(zip(keypointsList, np.array(list(encoder.transform([word])) * num_of_samples)))\n",
    "    return iter(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = generate_iterator(data, key = 'train')\n",
    "iter_test = generate_iterator(data, key = 'test')\n",
    "iter_val = generate_iterator(data, key = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiter_test = [x for x in iter_test]\n",
    "iiter_train = [x for x in iter_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Let's adapt the PSFDataset from human body movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([\n",
    "    #transforms.spatial.Crop(),\n",
    "#     transforms.spatial.Normalize(),\n",
    "#     transforms.SpatioTemporalPath(),\n",
    "#     transforms.temporal.MultiDelayedTransformation(2),\n",
    "    transforms.temporal.DyadicPathSignatures(dyadic_levels=2,\n",
    "                                             signature_level=4)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first steps, no transforms are introduced yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new dictionaries identical to 'data' that contain the normalized landmarks without aumenting \n",
    "#the training set\n",
    "\n",
    "    \n",
    "d_normalize = {} # normalized data set\n",
    "for i in data: \n",
    "    d_normalize[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in data[i].items())\n",
    "    \n",
    "d_rotate = {}    # rotated data set \n",
    "for i in data: \n",
    "    d_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate= {} # normalized and rotated data set\n",
    "for i in d_normalize:\n",
    "    d_normalize_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize[i].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new dictionary identical to 'data' that contains the normalized landmarks augmenting \n",
    "#the training set\n",
    "\n",
    "aug_data = ctransform.flip_train_augmentation(data)\n",
    "\n",
    "d_normalize_a = {} #normalized data set\n",
    "for i in data: \n",
    "    d_normalize_a[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in aug_data[i].items())\n",
    "    \n",
    "d_rotate_a = {}    #rotated data set \n",
    "for i in data: \n",
    "    d_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in aug_data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate_a= {} ## normalized and rotated data set\n",
    "for i in d_normalize_a:\n",
    "    d_normalize_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize_a[i].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [02:15, 147.73it/s]\n",
      "500it [00:03, 149.25it/s]\n",
      "500it [00:03, 148.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainingset elements: 20000\n",
      "Number of testset elements 500\n",
      "Dimension of feature vector: 6293\n",
      "Initial accuracy: 0.136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed01ae9925934d48967726067721e7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=20.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f7075ddbd4f218f93f7b8a162a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 0', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 48580.2039405132 \ttest_loss: 28021.020079488284 \tAccuracy: 0.472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0761ac0726314427ba1199ca6331da10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 1', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 32685.799406687325 \ttest_loss: 19498.63169997267 \tAccuracy: 0.474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77314308424e4d70915b4fc304c39284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 2', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 20813.82655459958 \ttest_loss: 14516.109644191272 \tAccuracy: 0.464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901cacfcdafa4df697a25e2f3dfe0bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 3', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 13212.019312656332 \ttest_loss: 10717.665438641707 \tAccuracy: 0.454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f8a41ba4b04335b061a79d9a358d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 4', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10833.994106986602 \ttest_loss: 9796.824044388442 \tAccuracy: 0.406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaee7f0f21584f56bff6a348c31551de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 5', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 9918.912431901903 \ttest_loss: 19504.204618753225 \tAccuracy: 0.378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c2372dcbba49d7973e29fe03c70dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 6', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10173.41507955105 \ttest_loss: 11266.701388351967 \tAccuracy: 0.412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0312772669d4fc8b50b8dd346184bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 7', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10465.056869145805 \ttest_loss: 12033.000988362097 \tAccuracy: 0.414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71285eec0c44e3aab3d8e72cdf4eff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 8', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10304.74219833627 \ttest_loss: 6673.528909753195 \tAccuracy: 0.506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dd992389da4cdbb52855351b35dfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 9', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10600.236625995169 \ttest_loss: 11928.17680062295 \tAccuracy: 0.434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74045a77425541f583e7050ff0156c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 10', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10722.188716111086 \ttest_loss: 10035.7393036805 \tAccuracy: 0.482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b1dd6781544598399b35b8adf92cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 11', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10635.006626469312 \ttest_loss: 9865.603900121607 \tAccuracy: 0.438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba4fd97cc4247a08be071d6366c1364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 12', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10961.288896983004 \ttest_loss: 12738.152557557562 \tAccuracy: 0.418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59514d4634d14e18a0f003f78d85270c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 13', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10572.715956308753 \ttest_loss: 10378.116811447353 \tAccuracy: 0.458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d249be02f1445298422d024168173b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 14', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 11091.845742233609 \ttest_loss: 9189.422829079424 \tAccuracy: 0.478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e59f193d19f4fd69203e4b058a617d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 15', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10790.263927097798 \ttest_loss: 13140.37324399416 \tAccuracy: 0.432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd33c21f2af46b2b56bc5d91ac112d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 16', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10932.71804278946 \ttest_loss: 10658.84329831092 \tAccuracy: 0.478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01f9d6f73e4a6b920db47cc9d55b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 17', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, outputs = ctransform.trainModelWithSpecificDataDict(dataDict = d_rotate_a, transforms = tr, EPOCHS = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
