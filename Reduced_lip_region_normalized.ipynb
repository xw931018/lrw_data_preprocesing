{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PATH='$PATH:/home/monga/.local/lib/python3.8/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import util\n",
    "from psfdataset import PSFDataset, transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import util_lip_data as util_lip\n",
    "import custom_transforms as ctransform\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 THOUGHT\n",
      "2 INVOLVED\n",
      "3 DIFFERENT\n",
      "4 MAYBE\n",
      "5 REPORT\n",
      "6 VICTIMS\n",
      "7 GETTING\n",
      "8 UNION\n",
      "9 MEDIA\n",
      "10 VIOLENCE\n",
      "-------------------------------\n",
      "dict_keys(['THOUGHT', 'INVOLVED', 'DIFFERENT', 'MAYBE', 'REPORT', 'VICTIMS', 'GETTING', 'UNION', 'MEDIA', 'VIOLENCE'])\n",
      "dict_keys(['val', 'test', 'train'])\n",
      "1000\n",
      "(29, 4, 2)\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "script to compute the number\n",
    "of jsons created for each\n",
    "word. The output is a .csv\n",
    "file where for each word the\n",
    "number of jsons corresponding\n",
    "to train, val, and test subsets\n",
    "is mentioned\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_rect_and_landmarks(rect, landmarks):\n",
    "    # converts and returns the face rectangle and landmarks\n",
    "    # in formats appropriate for the display function\n",
    "\n",
    "    x = rect[\"left\"]\n",
    "    y = rect[\"top\"]\n",
    "    w = rect[\"width\"]\n",
    "    h = rect[\"height\"]\n",
    "\n",
    "    if landmarks is not None:\n",
    "        temp_agg = list()\n",
    "        for i in range(len(landmarks)):\n",
    "            temp = list()\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"x\"])\n",
    "            temp.append(landmarks[\"point-\" + str(i+1)][\"y\"])\n",
    "            temp_agg.append(temp)\n",
    "        return (x, y, w, h), np.asarray(temp_agg)\n",
    "    else:\n",
    "        return (x, y, w, h), np.empty((0, 0))\n",
    "\n",
    "\n",
    "def choose_the_largest_face(faces_list):\n",
    "    if len(faces_list) == 1: \n",
    "        return faces_list[0]\n",
    "    \n",
    "    area_max = 0\n",
    "    area_max_id = 0\n",
    "    for i,face in enumerate(faces_list):\n",
    "        (face_rect,landmarks) = face\n",
    "        area = face_rect[2] * face_rect[3] # area = width * height\n",
    "        if area > area_max:\n",
    "            area_max = area\n",
    "            area_max_id = i\n",
    "    return faces_list[area_max_id]\n",
    "\n",
    "\n",
    "def load_one_json_file(filename, isDebug=False):\n",
    "    # load the metadata and facial landmarks\n",
    "\n",
    "    face_rect_list = []\n",
    "    landmarks_list = []\n",
    "    with open(filename) as f:\n",
    "        video_data_dict = json.load(f)\n",
    "        # extract duration\n",
    "        if video_data_dict[\"metaData\"] is not None:\n",
    "            duration = float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", video_data_dict[\"metaData\"][\"Duration\"])[0])\n",
    "            if isDebug:\n",
    "                print(\"duration metadata: %.3f\" % duration)\n",
    "\n",
    "        # extract frame information aggregated for all frames\n",
    "        agg_frame_data = video_data_dict[\"aggFrameInfo\"]  # list of frame-wise visual data\n",
    "\n",
    "        for frame_data in agg_frame_data:\n",
    "            n_faces = frame_data[\"numFaces\"]\n",
    "            if isDebug:\n",
    "                print(\"frame index: %d number of faces: %d\" % (frame_data[\"frameIndex\"], n_faces))\n",
    "            \n",
    "            if frame_data[\"facialAttributes\"] is not None:# if so, the n_faces should > 0 \n",
    "                faces_list = []\n",
    "                for attr in frame_data[\"facialAttributes\"]:\n",
    "                    face_idx = attr[\"faceIndex\"]\n",
    "                    face_rect, landmarks = get_rect_and_landmarks(attr[\"faceRectangle\"],\n",
    "                                                                  attr[\"faceLandmarks\"])\n",
    "                    faces_list.append((face_rect, landmarks))\n",
    "\n",
    "                face_rect_chosen, landmarks_chosen = choose_the_largest_face(faces_list)    \n",
    "                face_rect_list.append(face_rect_chosen)\n",
    "                landmarks_list.append(landmarks_chosen)\n",
    "    \n",
    "    face_rect_array = np.array(face_rect_list)\n",
    "    landmarks_array = np.array(landmarks_list)\n",
    "    return face_rect_array, landmarks_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json/lipread_mp4\"\n",
    "# or i_data = \"/cache/lrw/lipread_landmarks/dlib68_2d_sparse_json_defects_not_one_face/lipread_mp4\"\n",
    "selected_n_classes = 10 # the max is 500\n",
    "\n",
    "cnt = 0\n",
    "data = dict()\n",
    "\n",
    "for word in os.listdir(i_data):\n",
    "    if not word.startswith('.'):\n",
    "        cnt += 1\n",
    "        if cnt > selected_n_classes:\n",
    "            break\n",
    "        print(cnt,word)\n",
    "        splits = dict() # 'train' 'val' and 'test' sets\n",
    "        # print(\"analysing data for the word: '%s'\" % word)\n",
    "        p = os.path.join(i_data, word)\n",
    "        \n",
    "        for sub_dir in os.listdir(p):\n",
    "            if not sub_dir.startswith('.'):\n",
    "                # print(sub_dir)\n",
    "                p_sub = os.path.join(p, sub_dir)\n",
    "                for _, _, files in os.walk(p_sub):\n",
    "                    samples_list = []\n",
    "                    for filename in files:\n",
    "                        if filename.endswith('.json'):\n",
    "                            face_rect_array, landmarks_array = load_one_json_file(os.path.join(p_sub, filename))\n",
    "                            lip_region = []\n",
    "                            for j in range(len(landmarks_array)):\n",
    "                                lip_region.append(np.stack((landmarks_array[j][48],landmarks_array[j][51],landmarks_array[j][57],landmarks_array[j][54])))\n",
    "                            samples_list.append(np.array(lip_region))\n",
    "                    splits[sub_dir] = samples_list\n",
    "        data[word] = splits\n",
    "\n",
    "print('-------------------------------')\n",
    "print(data.keys()) # names of all the 'selected_n_classes' classes  \n",
    "print(data['THOUGHT'].keys()) # print the names of the 3 splits for the first class 'THOUGHT'\n",
    "print(len(data['THOUGHT']['train'])) # print the number of train samples of the first class\n",
    "print(data['THOUGHT']['train'][0].shape) # print the shape (29 frames, 68 landmarks, 2 coordinates) of the first training sample of the first class    \n",
    "print('-------------------------------')                     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Defining a LabelEncoder to transform text class to numberic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder= LabelEncoder()\n",
    "categories = list(data.keys())\n",
    "encoder.fit(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining my custom Data loader from above 'data'. We only need to properly define iterators for train/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_iterator(data, key = 'train', refLength = 29):\n",
    "    iter_list = []\n",
    "    x = 0\n",
    "    for word, keypointsOneWord in data.items():\n",
    "        \n",
    "\n",
    "        keypointsList = keypointsOneWord[key]\n",
    "        num_of_samples = len(keypointsList)\n",
    "        \n",
    "        ## There are some sample whose length is smaller than 29. We need to either delete it, \n",
    "        ## or extend it to 29 length for now. But this can be resolved if signature transform is introduced\n",
    "        for i in range(len(keypointsList)):\n",
    "             singleSample = keypointsList[i]\n",
    "             if len(singleSample) < refLength:\n",
    "                 singleSample = np.array(list(singleSample) + [singleSample[-1]] * (refLength - len(singleSample)))\n",
    "                 keypointsList[i] = singleSample\n",
    "        iter_list = iter_list + list(zip(keypointsList, np.array(list(encoder.transform([word])) * num_of_samples)))\n",
    "    return iter(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = generate_iterator(data, key = 'train')\n",
    "iter_test = generate_iterator(data, key = 'test')\n",
    "iter_val = generate_iterator(data, key = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiter_test = [x for x in iter_test]\n",
    "iiter_train = [x for x in iter_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Let's adapt the PSFDataset from human body movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([\n",
    "    #transforms.spatial.Crop(),\n",
    "#     transforms.spatial.Normalize(),\n",
    "#     transforms.SpatioTemporalPath(),\n",
    "#     transforms.temporal.MultiDelayedTransformation(2),\n",
    "    transforms.temporal.DyadicPathSignatures(dyadic_levels=2,\n",
    "                                             signature_level=4)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first steps, no transforms are introduced yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.9000e+01, 1.7200e+02, 3.0350e+03, 1.6084e+04, 9.0040e+03,\n",
       "        6.5800e+02, 1.7000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([-0.98279372, -0.72743472, -0.47207571, -0.21671671,  0.0386423 ,\n",
       "         0.2940013 ,  0.54936031,  0.80471931,  1.06007832,  1.31543732,\n",
       "         1.57079633]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ctransform.average_rot(data['THOUGHT']['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util_lip_data' from '/scratch/cdt_miniproject_2021/xiongw/lrw_data_preprocessing/util_lip_data.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(util_lip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new dictionaries identical to 'data' that contain the normalized landmarks without aumenting \n",
    "#the training set\n",
    "\n",
    "    \n",
    "d_normalize = {} # normalized data set\n",
    "for i in data: \n",
    "    d_normalize[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in data[i].items())\n",
    "    \n",
    "d_rotate = {}    # rotated data set \n",
    "for i in data: \n",
    "    d_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate= {} # normalized and rotated data set\n",
    "for i in d_normalize:\n",
    "    d_normalize_rotate[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize[i].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new dictionary identical to 'data' that contains the normalized landmarks augmenting \n",
    "#the training set\n",
    "\n",
    "aug_data = ctransform.flip_train_augmentation(data)\n",
    "\n",
    "d_normalize_a = {} #normalized data set\n",
    "for i in data: \n",
    "    d_normalize_a[i] = dict((k,ctransform.normalize_based_on_first_frame(v)) for k, v in aug_data[i].items())\n",
    "    \n",
    "d_rotate_a = {}    #rotated data set \n",
    "for i in data: \n",
    "    d_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in aug_data[i].items())\n",
    "        \n",
    "    \n",
    "d_normalize_rotate_a= {} ## normalized and rotated data set\n",
    "for i in d_normalize_a:\n",
    "    d_normalize_rotate_a[i] = dict((k,ctransform.rotate(v)) for k, v in d_normalize_a[i].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training model using trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:16, 624.16it/s]\n",
      "500it [00:00, 624.93it/s]\n",
      "500it [00:00, 626.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainingset elements: 10000\n",
      "Number of testset elements 500\n",
      "Dimension of feature vector: 6293\n",
      "Initial accuracy: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3cc02447cc4bccba1780fc5638a678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=20.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac0e546e50742d6a8a72fdbe86dda00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 0', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 2.316893288620241 \ttest_loss: 2.2135536281754065 \tAccuracy: 0.122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e765453c99424808b78803c392ff9612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 1', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 2.1921754857163935 \ttest_loss: 2.1238794605811706 \tAccuracy: 0.172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeec3deef974b00a81c9665d0babee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 2', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 2.059486415062188 \ttest_loss: 1.9619905836293843 \tAccuracy: 0.254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21894c4bdcc40d18aabe5825fc623d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 3', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.9473371364837102 \ttest_loss: 1.9029055372013777 \tAccuracy: 0.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c6e00b554f4c7b8f73f886938869ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 4', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.8569631240521873 \ttest_loss: 1.7810600978198687 \tAccuracy: 0.332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf42492b08494a6e87285bfb169fdb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 5', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, outputs = ctransform.trainModelWithSpecificDataDict(dataDict = d_normalize, transforms = tr, EPOCHS = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
